{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe54f65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:10:12.218861Z",
     "iopub.status.busy": "2025-11-02T10:10:12.218135Z",
     "iopub.status.idle": "2025-11-02T10:10:23.521029Z",
     "shell.execute_reply": "2025-11-02T10:10:23.520059Z"
    },
    "papermill": {
     "duration": 11.308908,
     "end_time": "2025-11-02T10:10:23.523137",
     "exception": false,
     "start_time": "2025-11-02T10:10:12.214229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 10:10:14.579173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-02 10:10:14.579298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-02 10:10:14.694732: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907ee92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:10:23.529627Z",
     "iopub.status.busy": "2025-11-02T10:10:23.528943Z",
     "iopub.status.idle": "2025-11-02T10:10:24.161352Z",
     "shell.execute_reply": "2025-11-02T10:10:24.160578Z"
    },
    "papermill": {
     "duration": 0.63752,
     "end_time": "2025-11-02T10:10:24.163348",
     "exception": false,
     "start_time": "2025-11-02T10:10:23.525828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_path = \"/kaggle/input/englishrussian-dictionary-for-machine-translate/rus.txt\"\n",
    "with open(text_path) as file:\n",
    "    lines = file.read().split(\"\\n\")[:-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19dbcb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:10:24.169142Z",
     "iopub.status.busy": "2025-11-02T10:10:24.168883Z",
     "iopub.status.idle": "2025-11-02T10:10:24.615315Z",
     "shell.execute_reply": "2025-11-02T10:10:24.614347Z"
    },
    "papermill": {
     "duration": 0.451196,
     "end_time": "2025-11-02T10:10:24.617040",
     "exception": false,
     "start_time": "2025-11-02T10:10:24.165844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs: 363386\n",
      "Sample pair: ('Things got messy.', '[start] Всё пошло наперекосяк. [end]')\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for line in lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) < 2:\n",
    "        continue  # Skip malformed lines\n",
    "    english, russian = parts[0], parts[1]\n",
    "    russian = \"[start] \" + russian.strip() + \" [end]\"\n",
    "    pairs.append((english.strip(), russian))\n",
    "\n",
    "print(f\"Total sentence pairs: {len(pairs)}\")\n",
    "print(\"Sample pair:\", random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000aaae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:10:24.623376Z",
     "iopub.status.busy": "2025-11-02T10:10:24.622826Z",
     "iopub.status.idle": "2025-11-02T10:10:24.826362Z",
     "shell.execute_reply": "2025-11-02T10:10:24.825376Z"
    },
    "papermill": {
     "duration": 0.20869,
     "end_time": "2025-11-02T10:10:24.828122",
     "exception": false,
     "start_time": "2025-11-02T10:10:24.619432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 290710, Val: 36338, Test: 36338\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "total = len(pairs)\n",
    "num_val = num_test = total // 10\n",
    "num_train = total - num_val - num_test\n",
    "\n",
    "train_pairs = pairs[:num_train]\n",
    "val_pairs = pairs[num_train:num_train + num_val]\n",
    "test_pairs = pairs[num_train + num_val:]\n",
    "\n",
    "print(f\"Train: {len(train_pairs)}, Val: {len(val_pairs)}, Test: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc765323",
   "metadata": {},
   "source": [
    "Data dibagi secara acak (dengan seed tetap untuk reprodusibilitas) menjadi 80% pelatihan, 10% validasi, dan 10% pengujian. Sebelum vektorisasi, teks dibersihkan dari tanda baca (kecuali kurung siku) dan diubah ke huruf kecil melalui fungsi custom_standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d365699f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:10:24.834353Z",
     "iopub.status.busy": "2025-11-02T10:10:24.834082Z",
     "iopub.status.idle": "2025-11-02T10:10:28.064643Z",
     "shell.execute_reply": "2025-11-02T10:10:28.063899Z"
    },
    "papermill": {
     "duration": 3.236148,
     "end_time": "2025-11-02T10:10:28.066752",
     "exception": false,
     "start_time": "2025-11-02T10:10:24.830604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "strip_chars = string.punctuation.replace(\"[\", \"\").replace(\"]\", \"\")  \n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    # Escape special regex chars in punctuation\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "# Vectorization layers\n",
    "vocab_size = 15000  # Lebih realistis untuk dataset kecil → hindari overfit\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,  # +1 for teacher forcing\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Adapt vocabulary only on training data\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_rus_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_eng_texts)\n",
    "target_vectorization.adapt(train_rus_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b8843",
   "metadata": {},
   "source": [
    "Dua lapisan TextVectorization digunakan: satu untuk sumber (Inggris) dan satu untuk target (Rusia). Keduanya dibatasi pada vocabulari 15.000 token dan panjang urutan tetap (20 untuk sumber, 21 untuk target guna mendukung teacher forcing). Pentingnya, adaptasi vocabulari dilakukan hanya pada data pelatihan untuk mencegah kebocoran data (data leakage) ke set validasi dan uji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feee5337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:10:28.073216Z",
     "iopub.status.busy": "2025-11-02T10:10:28.072940Z",
     "iopub.status.idle": "2025-11-02T10:10:28.086531Z",
     "shell.execute_reply": "2025-11-02T10:10:28.085737Z"
    },
    "papermill": {
     "duration": 0.018682,
     "end_time": "2025-11-02T10:10:28.088123",
     "exception": false,
     "start_time": "2025-11-02T10:10:28.069441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        return self.token_embeddings(inputs) + self.position_embeddings(positions)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        }\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]  # Expand for attention\n",
    "        attn_out = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        x = self.layernorm_1(inputs + attn_out)\n",
    "        proj_out = self.dense_proj(x)\n",
    "        return self.layernorm_2(x + proj_out)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\": self.num_heads}\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        i = tf.range(seq_len)[:, None]\n",
    "        j = tf.range(seq_len)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        return mask[None, :, :]  # Shape: (1, T, T)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = mask[:, None, :]  # (B, 1, T)\n",
    "            combined_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            combined_mask = causal_mask\n",
    "\n",
    "        attn1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out1 = self.layernorm_1(inputs + attn1)\n",
    "\n",
    "        attn2 = self.attention_2(\n",
    "            query=out1, value=encoder_outputs, key=encoder_outputs, attention_mask=combined_mask\n",
    "        )\n",
    "        out2 = self.layernorm_2(out1 + attn2)\n",
    "\n",
    "        proj = self.dense_proj(out2)\n",
    "        return self.layernorm_3(out2 + proj)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\": self.num_heads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d14d7",
   "metadata": {},
   "source": [
    "- PositionalEmbedding: Mengkompensasi ketiadaan informasi urutan dalam mekanisme self-attention dengan menambahkan embedding posisi ke embedding token.\n",
    "- TransformerEncoder: Menerapkan multi-head self-attention diikuti jaringan feed-forward dua lapis, dengan residual connection dan LayerNormalization untuk stabilitas pelatihan.\n",
    "- TransformerDecoder: Memiliki dua lapisan multi-head attention: (1) masked self-attention (menggunakan causal mask agar prediksi token ke-t hanya bergantung pada token sebelumnya), dan (2) encoder-decoder attention yang menghubungkan representasi sumber dan target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c80e3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:10:28.093889Z",
     "iopub.status.busy": "2025-11-02T10:10:28.093601Z",
     "iopub.status.idle": "2025-11-02T10:14:07.831751Z",
     "shell.execute_reply": "2025-11-02T10:14:07.830777Z"
    },
    "papermill": {
     "duration": 219.743398,
     "end_time": "2025-11-02T10:14:07.833777",
     "exception": false,
     "start_time": "2025-11-02T10:10:28.090379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - Loss: 9.7006, Acc: 0.0000\n",
      "\u001b[1m   2/4543\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:57\u001b[0m 66ms/step - accuracy: 0.1717 - loss: 9.2590       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762078245.915613      71 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1762078245.942974      71 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "W0000 00:00:1762078245.975219      71 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "W0000 00:00:1762078245.984250      71 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  50/4543\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:59\u001b[0m 40ms/step - accuracy: 0.6498 - loss: 5.0218Batch 50 - Loss: 3.2169, Acc: 0.7219\n",
      "\u001b[1m 100/4543\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:54\u001b[0m 39ms/step - accuracy: 0.6922 - loss: 3.9095Batch 100 - Loss: 2.5071, Acc: 0.7444\n",
      "\u001b[1m 150/4543\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:52\u001b[0m 39ms/step - accuracy: 0.7117 - loss: 3.3907Batch 150 - Loss: 2.2237, Acc: 0.7564\n",
      "\u001b[1m 200/4543\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:50\u001b[0m 39ms/step - accuracy: 0.7239 - loss: 3.0776Batch 200 - Loss: 2.0628, Acc: 0.7646\n",
      "\u001b[1m 250/4543\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:48\u001b[0m 39ms/step - accuracy: 0.7327 - loss: 2.8632Batch 250 - Loss: 1.9530, Acc: 0.7708\n",
      "\u001b[1m 300/4543\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46\u001b[0m 39ms/step - accuracy: 0.7395 - loss: 2.7045Batch 300 - Loss: 1.8692, Acc: 0.7760\n",
      "\u001b[1m 350/4543\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:44\u001b[0m 39ms/step - accuracy: 0.7450 - loss: 2.5807Batch 350 - Loss: 1.8078, Acc: 0.7795\n",
      "\u001b[1m 400/4543\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:42\u001b[0m 39ms/step - accuracy: 0.7495 - loss: 2.4806Batch 400 - Loss: 1.7518, Acc: 0.7833\n",
      "\u001b[1m 450/4543\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 39ms/step - accuracy: 0.7534 - loss: 2.3971Batch 450 - Loss: 1.7069, Acc: 0.7865\n",
      "\u001b[1m 500/4543\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:38\u001b[0m 39ms/step - accuracy: 0.7569 - loss: 2.3262Batch 500 - Loss: 1.6698, Acc: 0.7890\n",
      "\u001b[1m 550/4543\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:36\u001b[0m 39ms/step - accuracy: 0.7599 - loss: 2.2649Batch 550 - Loss: 1.6349, Acc: 0.7915\n",
      "\u001b[1m 600/4543\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 39ms/step - accuracy: 0.7626 - loss: 2.2112Batch 600 - Loss: 1.6047, Acc: 0.7938\n",
      "\u001b[1m 650/4543\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 39ms/step - accuracy: 0.7651 - loss: 2.1636Batch 650 - Loss: 1.5799, Acc: 0.7956\n",
      "\u001b[1m 700/4543\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 39ms/step - accuracy: 0.7674 - loss: 2.1210Batch 700 - Loss: 1.5559, Acc: 0.7973\n",
      "\u001b[1m 750/4543\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 39ms/step - accuracy: 0.7694 - loss: 2.0826Batch 750 - Loss: 1.5336, Acc: 0.7989\n",
      "\u001b[1m 800/4543\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 40ms/step - accuracy: 0.7713 - loss: 2.0476Batch 800 - Loss: 1.5132, Acc: 0.8005\n",
      "\u001b[1m 850/4543\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:26\u001b[0m 40ms/step - accuracy: 0.7731 - loss: 2.0156Batch 850 - Loss: 1.4932, Acc: 0.8020\n",
      "\u001b[1m 900/4543\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 40ms/step - accuracy: 0.7747 - loss: 1.9861Batch 900 - Loss: 1.4762, Acc: 0.8032\n",
      "\u001b[1m 950/4543\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 40ms/step - accuracy: 0.7762 - loss: 1.9589Batch 950 - Loss: 1.4599, Acc: 0.8046\n",
      "\u001b[1m1000/4543\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 40ms/step - accuracy: 0.7777 - loss: 1.9336Batch 1000 - Loss: 1.4452, Acc: 0.8056\n",
      "\u001b[1m1050/4543\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 40ms/step - accuracy: 0.7790 - loss: 1.9100Batch 1050 - Loss: 1.4294, Acc: 0.8069\n",
      "\u001b[1m1100/4543\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 40ms/step - accuracy: 0.7803 - loss: 1.8878Batch 1100 - Loss: 1.4156, Acc: 0.8079\n",
      "\u001b[1m1150/4543\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 40ms/step - accuracy: 0.7815 - loss: 1.8670Batch 1150 - Loss: 1.4017, Acc: 0.8090\n",
      "\u001b[1m1200/4543\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 40ms/step - accuracy: 0.7827 - loss: 1.8473Batch 1200 - Loss: 1.3897, Acc: 0.8099\n",
      "\u001b[1m1250/4543\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 40ms/step - accuracy: 0.7838 - loss: 1.8288Batch 1250 - Loss: 1.3778, Acc: 0.8108\n",
      "\u001b[1m1300/4543\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 40ms/step - accuracy: 0.7849 - loss: 1.8112Batch 1300 - Loss: 1.3669, Acc: 0.8116\n",
      "\u001b[1m1350/4543\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 40ms/step - accuracy: 0.7859 - loss: 1.7946Batch 1350 - Loss: 1.3563, Acc: 0.8125\n",
      "\u001b[1m1400/4543\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 40ms/step - accuracy: 0.7868 - loss: 1.7787Batch 1400 - Loss: 1.3461, Acc: 0.8132\n",
      "\u001b[1m1450/4543\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 40ms/step - accuracy: 0.7878 - loss: 1.7637Batch 1450 - Loss: 1.3369, Acc: 0.8140\n",
      "\u001b[1m1500/4543\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 40ms/step - accuracy: 0.7886 - loss: 1.7493Batch 1500 - Loss: 1.3276, Acc: 0.8147\n",
      "\u001b[1m1550/4543\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 40ms/step - accuracy: 0.7895 - loss: 1.7355Batch 1550 - Loss: 1.3179, Acc: 0.8155\n",
      "\u001b[1m1600/4543\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 40ms/step - accuracy: 0.7903 - loss: 1.7224Batch 1600 - Loss: 1.3092, Acc: 0.8162\n",
      "\u001b[1m1650/4543\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 40ms/step - accuracy: 0.7911 - loss: 1.7097Batch 1650 - Loss: 1.3000, Acc: 0.8169\n",
      "\u001b[1m1700/4543\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 40ms/step - accuracy: 0.7919 - loss: 1.6975Batch 1700 - Loss: 1.2916, Acc: 0.8176\n",
      "\u001b[1m1750/4543\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 40ms/step - accuracy: 0.7926 - loss: 1.6858Batch 1750 - Loss: 1.2834, Acc: 0.8183\n",
      "\u001b[1m1800/4543\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 40ms/step - accuracy: 0.7933 - loss: 1.6745Batch 1800 - Loss: 1.2757, Acc: 0.8189\n",
      "\u001b[1m1850/4543\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 40ms/step - accuracy: 0.7940 - loss: 1.6636Batch 1850 - Loss: 1.2682, Acc: 0.8195\n",
      "\u001b[1m1900/4543\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 40ms/step - accuracy: 0.7947 - loss: 1.6532Batch 1900 - Loss: 1.2614, Acc: 0.8200\n",
      "\u001b[1m1950/4543\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 40ms/step - accuracy: 0.7954 - loss: 1.6430Batch 1950 - Loss: 1.2542, Acc: 0.8206\n",
      "\u001b[1m2000/4543\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 40ms/step - accuracy: 0.7960 - loss: 1.6332Batch 2000 - Loss: 1.2472, Acc: 0.8212\n",
      "\u001b[1m2050/4543\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 40ms/step - accuracy: 0.7966 - loss: 1.6237Batch 2050 - Loss: 1.2406, Acc: 0.8217\n",
      "\u001b[1m2100/4543\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 40ms/step - accuracy: 0.7972 - loss: 1.6145Batch 2100 - Loss: 1.2342, Acc: 0.8222\n",
      "\u001b[1m2150/4543\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 40ms/step - accuracy: 0.7978 - loss: 1.6056Batch 2150 - Loss: 1.2281, Acc: 0.8227\n",
      "\u001b[1m2200/4543\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 40ms/step - accuracy: 0.7984 - loss: 1.5970Batch 2200 - Loss: 1.2220, Acc: 0.8232\n",
      "\u001b[1m2250/4543\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 40ms/step - accuracy: 0.7989 - loss: 1.5886Batch 2250 - Loss: 1.2162, Acc: 0.8236\n",
      "\u001b[1m2300/4543\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 40ms/step - accuracy: 0.7995 - loss: 1.5804Batch 2300 - Loss: 1.2106, Acc: 0.8241\n",
      "\u001b[1m2350/4543\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 40ms/step - accuracy: 0.8000 - loss: 1.5725Batch 2350 - Loss: 1.2047, Acc: 0.8246\n",
      "\u001b[1m2400/4543\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 40ms/step - accuracy: 0.8005 - loss: 1.5648Batch 2400 - Loss: 1.1991, Acc: 0.8250\n",
      "\u001b[1m2450/4543\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 40ms/step - accuracy: 0.8010 - loss: 1.5572Batch 2450 - Loss: 1.1936, Acc: 0.8255\n",
      "\u001b[1m2500/4543\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 40ms/step - accuracy: 0.8015 - loss: 1.5499Batch 2500 - Loss: 1.1885, Acc: 0.8259\n",
      "\u001b[1m2550/4543\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 40ms/step - accuracy: 0.8020 - loss: 1.5428Batch 2550 - Loss: 1.1835, Acc: 0.8263\n",
      "\u001b[1m2600/4543\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 40ms/step - accuracy: 0.8025 - loss: 1.5358Batch 2600 - Loss: 1.1781, Acc: 0.8267\n",
      "\u001b[1m2650/4543\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 40ms/step - accuracy: 0.8029 - loss: 1.5290Batch 2650 - Loss: 1.1733, Acc: 0.8271\n",
      "\u001b[1m2700/4543\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 40ms/step - accuracy: 0.8034 - loss: 1.5224Batch 2700 - Loss: 1.1686, Acc: 0.8275\n",
      "\u001b[1m2750/4543\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 40ms/step - accuracy: 0.8038 - loss: 1.5159Batch 2750 - Loss: 1.1638, Acc: 0.8279\n",
      "\u001b[1m2800/4543\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 40ms/step - accuracy: 0.8043 - loss: 1.5096Batch 2800 - Loss: 1.1591, Acc: 0.8283\n",
      "\u001b[1m2850/4543\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 40ms/step - accuracy: 0.8047 - loss: 1.5034Batch 2850 - Loss: 1.1546, Acc: 0.8287\n",
      "\u001b[1m2900/4543\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 40ms/step - accuracy: 0.8051 - loss: 1.4974Batch 2900 - Loss: 1.1502, Acc: 0.8290\n",
      "\u001b[1m2950/4543\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 40ms/step - accuracy: 0.8055 - loss: 1.4914Batch 2950 - Loss: 1.1459, Acc: 0.8294\n",
      "\u001b[1m3000/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 40ms/step - accuracy: 0.8059 - loss: 1.4856Batch 3000 - Loss: 1.1415, Acc: 0.8297\n",
      "\u001b[1m3050/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 40ms/step - accuracy: 0.8063 - loss: 1.4800 Batch 3050 - Loss: 1.1374, Acc: 0.8301\n",
      "\u001b[1m3100/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 40ms/step - accuracy: 0.8067 - loss: 1.4744Batch 3100 - Loss: 1.1330, Acc: 0.8304\n",
      "\u001b[1m3150/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 40ms/step - accuracy: 0.8071 - loss: 1.4690Batch 3150 - Loss: 1.1287, Acc: 0.8308\n",
      "\u001b[1m3200/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 40ms/step - accuracy: 0.8075 - loss: 1.4636Batch 3200 - Loss: 1.1244, Acc: 0.8312\n",
      "\u001b[1m3250/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 40ms/step - accuracy: 0.8078 - loss: 1.4584Batch 3250 - Loss: 1.1206, Acc: 0.8315\n",
      "\u001b[1m3300/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 40ms/step - accuracy: 0.8082 - loss: 1.4532Batch 3300 - Loss: 1.1165, Acc: 0.8318\n",
      "\u001b[1m3350/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 40ms/step - accuracy: 0.8085 - loss: 1.4482Batch 3350 - Loss: 1.1128, Acc: 0.8321\n",
      "\u001b[1m3400/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 40ms/step - accuracy: 0.8089 - loss: 1.4432Batch 3400 - Loss: 1.1090, Acc: 0.8325\n",
      "\u001b[1m3450/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m43s\u001b[0m 40ms/step - accuracy: 0.8092 - loss: 1.4383Batch 3450 - Loss: 1.1053, Acc: 0.8328\n",
      "\u001b[1m3500/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m41s\u001b[0m 40ms/step - accuracy: 0.8096 - loss: 1.4335Batch 3500 - Loss: 1.1018, Acc: 0.8331\n",
      "\u001b[1m3550/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m39s\u001b[0m 40ms/step - accuracy: 0.8099 - loss: 1.4288Batch 3550 - Loss: 1.0979, Acc: 0.8334\n",
      "\u001b[1m3600/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m37s\u001b[0m 40ms/step - accuracy: 0.8102 - loss: 1.4242Batch 3600 - Loss: 1.0945, Acc: 0.8337\n",
      "\u001b[1m3650/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m35s\u001b[0m 40ms/step - accuracy: 0.8106 - loss: 1.4197Batch 3650 - Loss: 1.0911, Acc: 0.8340\n",
      "\u001b[1m3700/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m33s\u001b[0m 40ms/step - accuracy: 0.8109 - loss: 1.4152Batch 3700 - Loss: 1.0878, Acc: 0.8343\n",
      "\u001b[1m3750/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.8112 - loss: 1.4108Batch 3750 - Loss: 1.0843, Acc: 0.8346\n",
      "\u001b[1m3800/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.8115 - loss: 1.4065Batch 3800 - Loss: 1.0811, Acc: 0.8349\n",
      "\u001b[1m3850/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m27s\u001b[0m 40ms/step - accuracy: 0.8118 - loss: 1.4023Batch 3850 - Loss: 1.0778, Acc: 0.8352\n",
      "\u001b[1m3900/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.8121 - loss: 1.3981Batch 3900 - Loss: 1.0747, Acc: 0.8354\n",
      "\u001b[1m3950/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m23s\u001b[0m 40ms/step - accuracy: 0.8124 - loss: 1.3940Batch 3950 - Loss: 1.0715, Acc: 0.8357\n",
      "\u001b[1m4000/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.8127 - loss: 1.3899Batch 4000 - Loss: 1.0684, Acc: 0.8360\n",
      "\u001b[1m4050/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.8130 - loss: 1.3859Batch 4050 - Loss: 1.0653, Acc: 0.8363\n",
      "\u001b[1m4100/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - accuracy: 0.8133 - loss: 1.3820Batch 4100 - Loss: 1.0621, Acc: 0.8366\n",
      "\u001b[1m4150/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.8136 - loss: 1.3781Batch 4150 - Loss: 1.0590, Acc: 0.8369\n",
      "\u001b[1m4200/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 40ms/step - accuracy: 0.8138 - loss: 1.3743Batch 4200 - Loss: 1.0562, Acc: 0.8371\n",
      "\u001b[1m4250/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 40ms/step - accuracy: 0.8141 - loss: 1.3706Batch 4250 - Loss: 1.0534, Acc: 0.8374\n",
      "\u001b[1m4300/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.8144 - loss: 1.3669Batch 4300 - Loss: 1.0505, Acc: 0.8376\n",
      "\u001b[1m4350/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.8146 - loss: 1.3632Batch 4350 - Loss: 1.0477, Acc: 0.8379\n",
      "\u001b[1m4400/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.8149 - loss: 1.3596Batch 4400 - Loss: 1.0448, Acc: 0.8381\n",
      "\u001b[1m4450/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.8152 - loss: 1.3561Batch 4450 - Loss: 1.0421, Acc: 0.8384\n",
      "\u001b[1m4500/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.8154 - loss: 1.3526Batch 4500 - Loss: 1.0394, Acc: 0.8386\n",
      "\u001b[1m4509/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.8155 - loss: 1.3519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1762078435.972297      69 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "W0000 00:00:1762078435.988883      69 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4543/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8157 - loss: 1.3496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1762078439.259833      71 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
      "W0000 00:00:1762078447.024542      71 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4543/4543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 44ms/step - accuracy: 0.8157 - loss: 1.3495 - val_accuracy: 0.8641 - val_loss: 0.7600\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 256\n",
    "dense_dim = 512      # Kurangi dari 2048 → lebih cepat & stabil untuk 1 epoch\n",
    "num_heads = 8\n",
    "batch_size = 64      # Efisien di Kaggle GPU/TPU\n",
    "\n",
    "# Build model\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"russian\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.1)(x)  # Kurangi dropout agar tidak terlalu agresif di 1 epoch\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Dataset pipeline\n",
    "def format_dataset(eng, rus):\n",
    "    eng = source_vectorization(eng)\n",
    "    rus = target_vectorization(rus)\n",
    "    return ({\"english\": eng, \"russian\": rus[:, :-1]}, rus[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, rus_texts = zip(*pairs)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(rus_texts)))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "# Callback untuk log loss & acc per batch (opsional, tapi sesuai permintaan)\n",
    "class BatchLogger(keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % 50 == 0:  # Print setiap 50 batch\n",
    "            print(f\"Batch {batch} - Loss: {logs['loss']:.4f}, Acc: {logs['accuracy']:.4f}\")\n",
    "\n",
    "# Train hanya 1 epoch\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=1,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[BatchLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52973331",
   "metadata": {},
   "source": [
    "Model memakai optimizer RMSprop (learning rate = 0.001) dan fungsi kerugian sparse categorical crossentropy, fungsi kerugian sparse categorical crossentropy, sesuai untuk prediksi indeks token diskrit. Pipeline data dibangun menggunakan tf.data untuk efisiensi: batching, transformasi input/output melalui format_dataset (yang menerapkan teacher forcing dengan menggeser target satu langkah), serta prefetching untuk paralelisasi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bba786",
   "metadata": {},
   "source": [
    "Model dilatih dengan callback BatchLogger yang mencatat training loss dan accuracy setiap 50 batch. Hasil akhir training 81,6 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae8e306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:14:08.057123Z",
     "iopub.status.busy": "2025-11-02T10:14:08.056559Z",
     "iopub.status.idle": "2025-11-02T10:14:12.081101Z",
     "shell.execute_reply": "2025-11-02T10:14:12.080122Z"
    },
    "papermill": {
     "duration": 4.134464,
     "end_time": "2025-11-02T10:14:12.082982",
     "exception": false,
     "start_time": "2025-11-02T10:14:07.948518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSLATION RESULTS ===\n",
      "EN: tom knows what we need\n",
      "RU: Том знает что нам нужно\n",
      "\n",
      "EN: how is it going\n",
      "RU: Как это\n",
      "\n",
      "EN: how are you\n",
      "RU: Как ты [UNK]\n",
      "\n",
      "EN: it was nice seeing you\n",
      "RU: Это было тебя с тобой\n",
      "\n",
      "EN: till next time\n",
      "RU: [UNK] до следующей неделе\n",
      "\n",
      "EN: go\n",
      "RU: Идите\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare lookup for decoding\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "index_to_word = dict(enumerate(target_vocab))\n",
    "\n",
    "def decode_sequence(input_sentence, max_len=20):\n",
    "    # Tokenize input\n",
    "    tokenized_input = source_vectorization([input_sentence])\n",
    "    decoded = \"[start]\"\n",
    "\n",
    "    for i in range(max_len):\n",
    "        # Tokenize current output (teacher forcing tidak dipakai saat inferensi)\n",
    "        target_seq = target_vectorization([decoded])[:, :-1]\n",
    "        # Predict next token\n",
    "        preds = model([tokenized_input, target_seq])\n",
    "        next_token_idx = tf.argmax(preds[0, i, :]).numpy()\n",
    "        next_word = index_to_word.get(next_token_idx, \"[UNK]\")\n",
    "        decoded += \" \" + next_word\n",
    "        if next_word == \"[end]\" or next_word == \"\":\n",
    "            break\n",
    "    # Clean output\n",
    "    return decoded.replace(\"[start]\", \"\").replace(\"[end]\", \"\").strip()\n",
    "\n",
    "# Test translation\n",
    "test_sentences = [\n",
    "    \"tom knows what we need\",\n",
    "    \"how is it going\",\n",
    "    \"how are you\",\n",
    "    \"it was nice seeing you\",\n",
    "    \"till next time\",\n",
    "    \"go\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== TRANSLATION RESULTS ===\")\n",
    "for eng in test_sentences:\n",
    "    rus_pred = decode_sequence(eng)\n",
    "    print(f\"EN: {eng}\")\n",
    "    print(f\"RU: {rus_pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1416132,
     "sourceId": 2345761,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 245.187811,
   "end_time": "2025-11-02T10:14:14.947060",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-02T10:10:09.759249",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
